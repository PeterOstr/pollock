{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-03T09:02:06.093373500Z",
     "start_time": "2024-02-03T09:01:59.814769800Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, split, regexp_replace\n",
    "from pyspark.sql.functions import col, split, explode, regexp_replace\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import lit, col, when\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "import os\n",
    "from pyspark.sql.functions import col, when, lit, isnan\n",
    "from pyspark.sql.functions import lit, nanvl, isnan, when, avg\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import CatBoostEncoder\n",
    "import pickle\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import DoubleType, ArrayType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "sklearn.set_config(transform_output='pandas')\n",
    "from pyspark.sql.functions import array\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession, types as T\n",
    "from pyspark.sql.functions import udf\n",
    "from ast import literal_eval\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "import os\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# from geospark.register import upload_jars\n",
    "# from geospark.register import GeoSparkRegistrator\n",
    "# from geospark.core.SpatialRDD import PolygonRDD, PointRDD\n",
    "# from geospark.core.enums import FileDataSplitter\n",
    "# from geospark.core.enums import IndexType\n",
    "# from geospark.core.geom.envelope import Envelope\n",
    "# from geospark.core.spatialOperator import RangeQuery\n",
    "# from geospark.core.formatMapper.shapefileParser import ShapefileReader\n",
    "# from geospark.sql.types import GeometryType\n",
    "# from geospark.register import sparkJVM\n",
    "# import re\n",
    "# upload_jars()\n",
    "#\n",
    "#\n",
    "# spark = SparkSession.builder.appName(\"Spark_v1\")    .config(\"spark.default.parallelism\", 176*3) \\\n",
    "#     .config(\"spark.sql.shuffle.partitions\", 176*3) \\\n",
    "#     .config(\"spark.executor.memory\", \"50g\") \\\n",
    "#     .config(\"spark.executor.cores\", \"5\") \\\n",
    "#     .config(\"spark.driver.memory\", \"10g\") \\\n",
    "#     .config(\"spark.driver.maxResultSize\", \"1300g\") \\\n",
    "#     .getOrCreate()\n",
    "import os\n",
    "\n",
    "# setup arguments\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0'\n",
    "\n",
    "# initialize spark\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, BooleanType\n",
    "\n",
    "\n",
    "ram = 16\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName('spark_first_run') \\\n",
    "    .config(\"spark.executor.memory\", f\"{ram}g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", f\"{ram}g\") \\\n",
    "    .config(\"spark.driver.memory\", f\"{ram}g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", f\"{ram}g\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"100000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "#    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "\n",
    "#    .config(\"spark.executor.memoryOverhead\", \"10g\") \\\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Register Sedona functions to Spark\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import t\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import udf, lit\n",
    "\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import json\n",
    "from pyspark.sql.functions import udf\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, FloatType\n",
    "from pyspark.sql.functions import desc\n",
    "from time import sleep\n",
    "from pyspark.sql import Row\n",
    "#from pyspark.sql.functions import min, max\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import split, explode, lower, trim\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, count, col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import monotonically_increasing_id, concat, col, lit, cast\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import collect_list, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import initcap\n",
    "import re\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "# Get PySpark version\n",
    "print(\"PySpark version:\", pyspark.__version__)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T09:02:06.108378700Z",
     "start_time": "2024-02-03T09:02:06.095376300Z"
    }
   },
   "id": "c2e99182d25d3fb8",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "this for terminal\n",
    "setx JAVA_HOME \"C:\\Program Files\\Java\\jdk-11\"\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67b5b0745faf77cf"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      message|\n",
      "+-------------+\n",
      "|Hello, World!|\n",
      "+-------------+\n"
     ]
    }
   ],
   "source": [
    "# # Create a SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"HelloWorld\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# Create a DataFrame with a single column named \"message\" and a single row containing the string \"Hello, World!\"\n",
    "data = [(\"Hello, World!\",)]\n",
    "df = spark.createDataFrame(data, [\"message\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T09:02:22.552635600Z",
     "start_time": "2024-02-03T09:02:07.695478800Z"
    }
   },
   "id": "df544333e122accf",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "# Get PySpark version\n",
    "print(\"PySpark version:\", pyspark.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T21:42:35.485228600Z",
     "start_time": "2024-02-01T21:42:35.460233500Z"
    }
   },
   "id": "ccc125034d6a4110",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T20:09:44.000137100Z",
     "start_time": "2024-02-02T20:09:43.992124300Z"
    }
   },
   "id": "c2837ddad62d4ee4",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n"
     ]
    }
   ],
   "source": [
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.sql('''select 'spark' as hello ''')\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T20:06:06.987852800Z",
     "start_time": "2024-02-02T20:06:05.785858100Z"
    }
   },
   "id": "ddbdf7cff9d9b984",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mSpark\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mImpala\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m3.2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m2.2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\kanagawa\\lib\\site-packages\\pyspark\\sql\\session.py:1443\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[1;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[0;32m   1438\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[0;32m   1439\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[0;32m   1440\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[0;32m   1441\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[0;32m   1442\u001B[0m     )\n\u001B[1;32m-> 1443\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1444\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m   1445\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\kanagawa\\lib\\site-packages\\pyspark\\sql\\session.py:1485\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[1;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[0;32m   1483\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n\u001B[0;32m   1484\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1485\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromLocal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1486\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1487\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\kanagawa\\lib\\site-packages\\pyspark\\sql\\session.py:1116\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[1;34m(self, data, schema)\u001B[0m\n\u001B[0;32m   1114\u001B[0m \u001B[38;5;66;03m# convert python objects to sql data\u001B[39;00m\n\u001B[0;32m   1115\u001B[0m internal_data \u001B[38;5;241m=\u001B[39m [struct\u001B[38;5;241m.\u001B[39mtoInternal(row) \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m tupled_data]\n\u001B[1;32m-> 1116\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparallelize\u001B[49m\u001B[43m(\u001B[49m\u001B[43minternal_data\u001B[49m\u001B[43m)\u001B[49m, struct\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\kanagawa\\lib\\site-packages\\pyspark\\context.py:783\u001B[0m, in \u001B[0;36mSparkContext.parallelize\u001B[1;34m(self, c, numSlices)\u001B[0m\n\u001B[0;32m    751\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mparallelize\u001B[39m(\u001B[38;5;28mself\u001B[39m, c: Iterable[T], numSlices: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RDD[T]:\n\u001B[0;32m    752\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    753\u001B[0m \u001B[38;5;124;03m    Distribute a local Python collection to form an RDD. Using range\u001B[39;00m\n\u001B[0;32m    754\u001B[0m \u001B[38;5;124;03m    is recommended if the input represents a range for performance.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    781\u001B[0m \u001B[38;5;124;03m    [['a'], ['b', 'c']]\u001B[39;00m\n\u001B[0;32m    782\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 783\u001B[0m     numSlices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(numSlices) \u001B[38;5;28;01mif\u001B[39;00m numSlices \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdefaultParallelism\u001B[49m\n\u001B[0;32m    784\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mrange\u001B[39m):\n\u001B[0;32m    785\u001B[0m         size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(c)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\kanagawa\\lib\\site-packages\\pyspark\\context.py:630\u001B[0m, in \u001B[0;36mSparkContext.defaultParallelism\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    618\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m    619\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefaultParallelism\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m    620\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    621\u001B[0m \u001B[38;5;124;03m    Default level of parallelism to use when not given by user (e.g. for reduce tasks)\u001B[39;00m\n\u001B[0;32m    622\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;124;03m    True\u001B[39;00m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msc\u001B[49m()\u001B[38;5;241m.\u001B[39mdefaultParallelism()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Spark\", \"Impala\"), (\"3.2\", \"2.2\")])\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T09:02:34.341141200Z",
     "start_time": "2024-02-03T09:02:33.344617300Z"
    }
   },
   "id": "50a46a9982413eef",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Create DataFrame in PySpark Shell\u001B[39;00m\n\u001B[0;32m      2\u001B[0m data \u001B[38;5;241m=\u001B[39m [(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJava\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m20000\u001B[39m\u001B[38;5;124m\"\u001B[39m), (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m100000\u001B[39m\u001B[38;5;124m\"\u001B[39m), (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mScala\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m3000\u001B[39m\u001B[38;5;124m\"\u001B[39m)]\n\u001B[1;32m----> 3\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\kanagawa\\lib\\site-packages\\pyspark\\sql\\session.py:1443\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[1;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[0;32m   1438\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[0;32m   1439\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[0;32m   1440\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[0;32m   1441\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[0;32m   1442\u001B[0m     )\n\u001B[1;32m-> 1443\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1444\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m   1445\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\kanagawa\\lib\\site-packages\\pyspark\\sql\\session.py:1485\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[1;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[0;32m   1483\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n\u001B[0;32m   1484\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1485\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromLocal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1486\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1487\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\kanagawa\\lib\\site-packages\\pyspark\\sql\\session.py:1116\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[1;34m(self, data, schema)\u001B[0m\n\u001B[0;32m   1114\u001B[0m \u001B[38;5;66;03m# convert python objects to sql data\u001B[39;00m\n\u001B[0;32m   1115\u001B[0m internal_data \u001B[38;5;241m=\u001B[39m [struct\u001B[38;5;241m.\u001B[39mtoInternal(row) \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m tupled_data]\n\u001B[1;32m-> 1116\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparallelize\u001B[49m\u001B[43m(\u001B[49m\u001B[43minternal_data\u001B[49m\u001B[43m)\u001B[49m, struct\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\kanagawa\\lib\\site-packages\\pyspark\\context.py:783\u001B[0m, in \u001B[0;36mSparkContext.parallelize\u001B[1;34m(self, c, numSlices)\u001B[0m\n\u001B[0;32m    751\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mparallelize\u001B[39m(\u001B[38;5;28mself\u001B[39m, c: Iterable[T], numSlices: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RDD[T]:\n\u001B[0;32m    752\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    753\u001B[0m \u001B[38;5;124;03m    Distribute a local Python collection to form an RDD. Using range\u001B[39;00m\n\u001B[0;32m    754\u001B[0m \u001B[38;5;124;03m    is recommended if the input represents a range for performance.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    781\u001B[0m \u001B[38;5;124;03m    [['a'], ['b', 'c']]\u001B[39;00m\n\u001B[0;32m    782\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 783\u001B[0m     numSlices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(numSlices) \u001B[38;5;28;01mif\u001B[39;00m numSlices \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdefaultParallelism\u001B[49m\n\u001B[0;32m    784\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mrange\u001B[39m):\n\u001B[0;32m    785\u001B[0m         size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(c)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\kanagawa\\lib\\site-packages\\pyspark\\context.py:630\u001B[0m, in \u001B[0;36mSparkContext.defaultParallelism\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    618\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m    619\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefaultParallelism\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m    620\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    621\u001B[0m \u001B[38;5;124;03m    Default level of parallelism to use when not given by user (e.g. for reduce tasks)\u001B[39;00m\n\u001B[0;32m    622\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;124;03m    True\u001B[39;00m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msc\u001B[49m()\u001B[38;5;241m.\u001B[39mdefaultParallelism()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "# Create DataFrame in PySpark Shell\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T09:03:03.725721800Z",
     "start_time": "2024-02-03T09:03:03.638720200Z"
    }
   },
   "id": "2ba3a394865ae80e",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('PySpark_Tutorial') \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T20:11:23.438451500Z",
     "start_time": "2024-02-02T20:11:23.420452100Z"
    }
   },
   "id": "81731e1975296a89",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b151574a08f8b8fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
